{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37b31e6",
   "metadata": {},
   "source": [
    "## Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ab6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import cv2\n",
    "import gc\n",
    "from tqdm import tqdm,trange\n",
    "\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329a92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usefull for better outputs visualization\n",
    "PURPLE = \"\\033[95m\"\n",
    "CYAN = \"\\033[96m\"\n",
    "DARKCYAN = \"\\033[36m\"\n",
    "BLUE = \"\\033[94m\"\n",
    "GREEN = \"\\033[92m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "RED = \"\\033[91m\"\n",
    "BOLD = \"\\033[1m\"\n",
    "UNDERLINE = \"\\033[4m\"\n",
    "END = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7564cc00",
   "metadata": {},
   "source": [
    "### Project settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b42f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_unsplitted = \"dataset/unsplitted\"\n",
    "save_dir_splitted = \"dataset/splitted\"\n",
    "data_dir = \"dataset/splitted\"\n",
    "models_dir =\"models/\" #the resuls will be saved here\n",
    "\n",
    "os.makedirs(models_dir,exist_ok=True)\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "patch_size = 128\n",
    "n_samples_train = 40\n",
    "n_samples_valtest = 40\n",
    "batch_size = 75\n",
    "\n",
    "\n",
    "grayscale= False\n",
    "resize_to= None\n",
    "fft = False\n",
    "\n",
    "\n",
    "learning_rate= 0.001\n",
    "num_epochs = 70\n",
    "\n",
    "\n",
    "LOAD_DATASET = True\n",
    "DATASET_ALREADY_DOWNLOADED = True \n",
    "DATASET_ALREADY_UNZIPPED = True\n",
    "\n",
    "LOAD_MODEL = True \n",
    "MODEL_ALREADY_DOWNLOADED = True\n",
    "MODEL_ALREADY_UNZIPPED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b6f851",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1385d",
   "metadata": {},
   "source": [
    "### Dataset splitting and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30151b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"CLL\", \"FL\", \"MCL\"]\n",
    "id_to_name = {0:\"CLL\",1:\"FL\",2:\"MCL\"}\n",
    "name_to_id = {\"CLL\":0,\"FL\":1,\"MCL\":2}\n",
    "\n",
    "if(not LOAD_DATASET):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i, cl in enumerate(classes):\n",
    "        path = os.path.join(data_dir_unsplitted, cl)\n",
    "        images = os.listdir(path)\n",
    "        for img in tqdm(images,desc=cl):\n",
    "            img_path = os.path.join(path, img)\n",
    "            img = cv2.imread(img_path).astype(np.uint8)\n",
    "            \n",
    "            data.append(img)\n",
    "            labels.append(i) \n",
    "\n",
    "\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.125, random_state=42)\n",
    "\n",
    "    print(\"Train samples:\", len(train_data))\n",
    "    print(\"Test samples:\", len(test_data))\n",
    "    print(\"Validation samples:\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17f646",
   "metadata": {},
   "source": [
    "### Saving the splitted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a42d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, patch_size, n_samples = 40):\n",
    "    h, w, _ = image.shape\n",
    "    h_eff = h - patch_size\n",
    "    w_eff = w - patch_size\n",
    "    samples = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        h_vertex = np.random.randint(h_eff)\n",
    "        w_vertex = np.random.randint(w_eff)\n",
    "        samples.append(image[h_vertex:h_vertex+patch_size, w_vertex:w_vertex+patch_size, :])\n",
    "    \n",
    "    samples = np.stack(samples)\n",
    "    return samples   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd4d2b3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SAVING THE DATASET\n",
    "if(not LOAD_DATASET):\n",
    "    #TRAIN\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"train\",\"images\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"train\",\"images_patch\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"train\",\"labels\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"train\",\"labels_patch\"),exist_ok=True)\n",
    "     \n",
    "    train_labels_dict={}\n",
    "    train_labels_patch_dict={}\n",
    "    \n",
    "    for i, image in enumerate(tqdm(train_data,desc = \"Train\")):                       #images\n",
    "        file_name = str(i)+\"_train\"+\".png\"\n",
    "        \n",
    "        path = os.path.join(save_dir_splitted,\"train\",\"images\",file_name)\n",
    "        train_labels_dict[file_name] = int(train_labels[i])\n",
    "        cv2.imwrite(path,image.astype(np.float64))\n",
    "        \n",
    "        image_patch = crop_image(image, patch_size, n_samples_train)\n",
    "        for j in range(len(image_patch)):\n",
    "            file_name = str(i)+\"-\"+str(j)+\"_train\"+\".png\"\n",
    "            path = os.path.join(save_dir_splitted,\"train\",\"images_patch\",file_name)\n",
    "            train_labels_patch_dict[file_name] = int(train_labels[i])\n",
    "            cv2.imwrite(path,image_patch[j].astype(np.float64))\n",
    "       \n",
    "    \n",
    "    \n",
    "    with open(os.path.join(save_dir_splitted,\"train\",\"labels\",\"train.json\"), 'w') as f:\n",
    "        json.dump(train_labels_dict, f)#labels\n",
    "    with open(os.path.join(save_dir_splitted,\"train\",\"labels_patch\",\"train_patch.json\"), 'w') as f:\n",
    "        json.dump(train_labels_patch_dict, f)#labels\n",
    "\n",
    "    #TEST\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"test\",\"images\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"test\",\"images_patch\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"test\",\"labels\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"test\",\"labels_patch\"),exist_ok=True)\n",
    "\n",
    "    test_labels_dict={}\n",
    "    test_labels_patch_dict={}\n",
    "    \n",
    "    for i, image in enumerate(tqdm(test_data,desc = \"Test\")):                      #images\n",
    "        file_name = str(i)+\"_test\"+\".png\"\n",
    "        path = os.path.join(save_dir_splitted,\"test\",\"images\",file_name)\n",
    "        \n",
    "        cv2.imwrite(path,image.astype(np.float64))\n",
    "        test_labels_dict[file_name] = int(test_labels[i]) \n",
    "        \n",
    "        image_patch = crop_image(image, patch_size, n_samples_valtest)\n",
    "        for j in range(len(image_patch)):\n",
    "            file_name = str(i)+\"-\"+str(j)+\"_test\"+\".png\"\n",
    "            path = os.path.join(save_dir_splitted,\"test\",\"images_patch\",file_name)\n",
    "            test_labels_patch_dict[file_name] = int(test_labels[i])\n",
    "            cv2.imwrite(path,image_patch[j].astype(np.float64))\n",
    "\n",
    "    with open(os.path.join(save_dir_splitted,\"test\",\"labels\",\"test.json\"), 'w') as f:\n",
    "        json.dump(test_labels_dict, f)#labels\n",
    "    with open(os.path.join(save_dir_splitted,\"test\",\"labels_patch\",\"test_patch.json\"), 'w') as f:\n",
    "        json.dump(test_labels_patch_dict, f)#labels\n",
    "        \n",
    "    #VALIDATION\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"val\",\"images\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"val\",\"images_patch\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"val\",\"labels\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"val\",\"labels_patch\"),exist_ok=True)\n",
    "    \n",
    "    val_labels_dict = {}\n",
    "    val_labels_patch_dict = {}\n",
    "    \n",
    "    for i, image in enumerate(tqdm(val_data,desc = \"Validation\")):               #images\n",
    "        file_name = str(i)+\"_val\"+\".png\"\n",
    "        path = os.path.join(save_dir_splitted,\"val\",\"images\",file_name)\n",
    "        \n",
    "        cv2.imwrite(path,image.astype(np.float64))\n",
    "        val_labels_dict[file_name] = int(val_labels[i])\n",
    "        \n",
    "        image_patch = crop_image(image, patch_size, n_samples_valtest)\n",
    "        for j in range(len(image_patch)):\n",
    "            file_name = str(i)+\"-\"+str(j)+\"_val\"+\".png\"\n",
    "            path = os.path.join(save_dir_splitted,\"val\",\"images_patch\",file_name)\n",
    "            val_labels_patch_dict[file_name] = int(val_labels[i])\n",
    "            cv2.imwrite(path,image_patch[j].astype(np.float64))\n",
    "        \n",
    "    with open(os.path.join(save_dir_splitted,\"val\",\"labels\",\"val.json\"), 'w') as f:\n",
    "        json.dump(val_labels_dict, f)#labels\n",
    "    with open(os.path.join(save_dir_splitted,\"val\",\"labels_patch\",\"val_patch.json\"), 'w') as f:\n",
    "        json.dump(val_labels_patch_dict, f)#labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab21346c",
   "metadata": {},
   "source": [
    "### Dataloaders definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a1b860",
   "metadata": {},
   "source": [
    "Notice that the labels are saved into a numpy array, while the images are read directly from the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb6438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDAdataset(Dataset):\n",
    "    def __init__(self, root_dir, labels, train = False, transform=None, grayscale= False, resize_to= None, fft = False):\n",
    "        self.root_dir = root_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.grayscale=grayscale\n",
    "        self.fft = fft\n",
    "        self.resize_to = resize_to\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.root_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.listdir(self.root_dir)[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        \n",
    "        if self.grayscale or self.fft:\n",
    "            image = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "        else:\n",
    "            image = cv2.imread(img_path)\n",
    "\n",
    "        \n",
    "        if self.train:\n",
    "            image = image.astype(np.uint8) + np.random.normal(0, 2, image.shape).astype(np.uint8)\n",
    "        \n",
    "        if self.resize_to is not None:\n",
    "            image = cv2.resize(image, resize_to, interpolation= cv2.INTER_LINEAR)\n",
    "            \n",
    "        if self.fft:\n",
    "            image = np.fft.fft2(image)\n",
    "            fshift = np.fft.fftshift(image)\n",
    "            magnitude_spectrum = 20*np.log(np.abs(fshift))\n",
    "            image = magnitude_spectrum\n",
    "        \n",
    "\n",
    "    \n",
    "        #labels smoothing\n",
    "        label = 0.1*np.ones(3)\n",
    "        label[self.labels[img_name]] = 0.8\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform((image/255.).astype(np.float32))\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ce0dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDAdatasetPatches(Dataset):\n",
    "    def __init__(self, root_dir,patch_dir, labels, train = False, grayscale= False, fft = False):\n",
    "        self.root_dir = root_dir\n",
    "        self.patch_dir = patch_dir\n",
    "        self.labels = labels\n",
    "        self.train = train\n",
    "        self.grayscale=grayscale\n",
    "        self.fft = fft\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.root_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.listdir(self.root_dir)[idx]\n",
    "        img_id = int(img_name.split(\"_\")[0])\n",
    "        \n",
    "        patches_list = os.listdir(self.patch_dir)\n",
    "        patches_image_list = list(filter(lambda name: int(name.split(\"-\")[0]) == img_id, patches_list))\n",
    "        \n",
    "        all_patches = []\n",
    "        for patch in patches_image_list:\n",
    "            patch_path = os.path.join(self.patch_dir, patch)\n",
    "            \n",
    "\n",
    "            if self.grayscale or self.fft:\n",
    "                image = cv2.imread(patch_path,cv2.IMREAD_GRAYSCALE)\n",
    "                channels = 1\n",
    "            \n",
    "            else:\n",
    "                image = cv2.imread(patch_path)\n",
    "                channels = 3\n",
    "\n",
    "            if self.train:\n",
    "                image = image.astype(np.uint8) + np.random.normal(0, 2, image.shape).astype(np.uint8)\n",
    "\n",
    "            if self.fft:\n",
    "                image = np.fft.fft2(image)\n",
    "                fshift = np.fft.fftshift(image)\n",
    "                magnitude_spectrum = 20*np.log(np.abs(fshift))\n",
    "                image = magnitude_spectrum\n",
    "            \n",
    "            image = (image/255.).astype(np.float32)\n",
    "            image = image.reshape(image.shape[0],image.shape[1],channels)\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "            all_patches.append(image.reshape(1,image.shape[0],image.shape[1],image.shape[2]))\n",
    "\n",
    "        all_patches=np.stack(all_patches)\n",
    "        \n",
    "        #labels smoothing\n",
    "        label = 0.1*np.ones(3)\n",
    "        label[self.labels[img_name]] = 0.8\n",
    "\n",
    "        return all_patches, label.reshape(1,label.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26887536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating the train set list:  56%|█████████████████████████▌                    | 5804/10440 [01:10<01:02, 74.68it/s]"
     ]
    }
   ],
   "source": [
    "if(LOAD_DATASET):\n",
    "    \n",
    "    if(not (DATASET_ALREADY_DOWNLOADED or DATASET_ALREADY_UNZIPPED)):\n",
    "        print(BOLD+\"Download and the dataset\"+END)\n",
    "        \n",
    "        url = 'https://drive.google.com/u/0/uc?id=1gz9yCLtRA7Do9Or1M4z0q9IojLMJBie0&export=download'\n",
    "        output = 'dataset.zip'\n",
    "        gdown.download(url, output, quiet=False)\n",
    "        \n",
    "    if(not DATASET_ALREADY_UNZIPPED):\n",
    "        print(BOLD+\"Extract the dataset\"+END)\n",
    "        \n",
    "        with zipfile.ZipFile(\"dataset.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./dataset/\")\n",
    "\n",
    "        \n",
    "#convert to tensor each image\n",
    "transform = transforms.Compose([transforms.ToTensor()])    \n",
    "\n",
    "\n",
    "#TRAIN DATASET AND DATALOADER\n",
    "train_dir = os.path.join(data_dir,\"train\",\"images_patch\")#images\n",
    "train_labels_dir = os.path.join(data_dir,\"train\",\"labels_patch\",\"train_patch.json\")\n",
    "with open(train_labels_dir) as f:\n",
    "    train_labels = json.load(f)#labels\n",
    "\n",
    "train_dataset = HDAdataset(train_dir,train_labels,train = True, transform=transform\n",
    "                           ,grayscale=grayscale,fft=fft)\n",
    "\n",
    "#using a list to increase the training speed since the data fit in the ram\n",
    "train_data = [data for data in tqdm(train_dataset,desc=\"Generating the train set list\")] \n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "#TEST DATASET\n",
    "test_dir = os.path.join(data_dir,\"test\",\"images\")#images\n",
    "test_dir_patches = os.path.join(data_dir,\"test\",\"images_patch\")#patches\n",
    "test_labels_dir = os.path.join(data_dir,\"test\",\"labels\",\"test.json\")\n",
    "with open(test_labels_dir) as f:\n",
    "    test_labels = json.load(f)#labels\n",
    "\n",
    "test_dataset = HDAdatasetPatches(test_dir,test_dir_patches,test_labels,train = False,\n",
    "                          grayscale=grayscale,fft=fft)\n",
    "\n",
    "\n",
    "#VALIDATION DATASET\n",
    "val_dir = os.path.join(data_dir,\"val\",\"images\")#images\n",
    "val_dir_patches = os.path.join(data_dir,\"val\",\"images_patch\")#patches\n",
    "val_labels_dir = os.path.join(data_dir,\"val\",\"labels\",\"val.json\")\n",
    "with open(val_labels_dir) as f:\n",
    "    val_labels = json.load(f)#labels\n",
    "\n",
    "val_dataset = HDAdatasetPatches(val_dir,val_dir_patches,val_labels,train = False,\n",
    "                          grayscale=grayscale,fft=fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weight(dataset):\n",
    "    count = {0:0,1:0,2:0}\n",
    "    for data in tqdm(dataset,desc = \"Computing class weight\"):\n",
    "        label = data[1]\n",
    "        count[label.argmax()]= count[label.argmax()]+1\n",
    "    \n",
    "    count = np.array([count[0],count[1],count[2]])\n",
    "    total = count.sum()\n",
    "    return count/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69af862",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = compute_weight(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da722799",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5599d",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684cf15",
   "metadata": {},
   "source": [
    "### Models definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        #encoder\n",
    "        self.convE1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3,3), stride=(2,2), padding=(0,0))\n",
    "        self.convE2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3,3), stride=(2,2), padding=(0,0))\n",
    "        self.convE3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), stride=(2,2), padding=(0,0))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.BNE16 = nn.BatchNorm2d(16)\n",
    "        self.BNE32 = nn.BatchNorm2d(32)\n",
    "        self.BNE64 = nn.BatchNorm2d(64)\n",
    "        self.linearE1 = nn.Linear(14400,50)\n",
    "        self.linearE2 = nn.Linear(50,3)\n",
    "        self.BNE150linear = nn.BatchNorm1d(50)\n",
    "        \n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        \n",
    "        #decoder\n",
    "        \n",
    "        self.convD1 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=(3,3), stride=(2,2), padding=(0,0))\n",
    "        self.convD2 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=(3,3), stride=(2,2), padding=(0,0))\n",
    "        self.convD3 = nn.ConvTranspose2d(in_channels=16, out_channels=3, kernel_size=(3,3), \n",
    "                                         stride=(2,2), padding=(0,0), dilation=1,output_padding=(1,1))\n",
    "        self.reshape = nn.Unflatten(-1, (64,15,15))\n",
    "        self.BND16 = nn.BatchNorm2d(16)\n",
    "        self.BND32 = nn.BatchNorm2d(32)\n",
    "        self.linearD1 = nn.Linear(3,50)\n",
    "        self.linearD2 = nn.Linear(50,14400)\n",
    "        self.BND150linear = nn.BatchNorm1d(50)\n",
    "        self.BND16klinear = nn.BatchNorm1d(14400)\n",
    "        \n",
    "        self.out = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x1 = self.act(self.BNE16(self.convE1(x)))\n",
    "        x2 = self.act(self.BNE32(self.convE2(x1)))\n",
    "        x3 = self.act(self.BNE64(self.convE3(x2)))\n",
    "        x3linear = self.flatten(x3)\n",
    "        x4 = self.drop(self.act(self.BNE150linear(self.linearE1(x3linear))))\n",
    "        x5 = self.linearE2(x4)\n",
    "\n",
    "        x = self.act(self.BND150linear(self.linearD1(x5)))\n",
    "        x = self.act(self.BND16klinear(self.linearD2(x+x4)))\n",
    "        x = self.reshape(x)\n",
    "        x = self.act(self.BND32(self.convD1(x+x3)))\n",
    "        x = self.act(self.BND16(self.convD2(x+x2)))\n",
    "        x = self.out(self.convD3(x+x1))\n",
    "\n",
    "        return x,self.softmax(x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c567e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(BOLD+\"You are working on: \"+END+str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f5a37",
   "metadata": {},
   "source": [
    "### Loading of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(LOAD_MODEL):\n",
    "    \n",
    "    if(not (MODEL_ALREADY_DOWNLOADED or MODEL_ALREADY_UNZIPPED)):\n",
    "        print(BOLD+\"Download the model\"+END)\n",
    "        \n",
    "        url = 'https://drive.google.com/u/0/uc?id=1bRwmud6xXNNSQvKo2J_WBCe7__ErAHim&export=download'\n",
    "        output = 'models.zip'\n",
    "        gdown.download(url, output, quiet=False)\n",
    "        \n",
    "    if(not MODEL_ALREADY_UNZIPPED):\n",
    "        \n",
    "        print(BOLD+\"Extract the model\"+END)\n",
    "        \n",
    "        with zipfile.ZipFile(\"models.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6460ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not LOAD_MODEL):\n",
    "    autoencoder = Autoencoder().to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1214cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(autoencoder, (3, 128, 128), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931580de",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "The accuracy in the training is patch wise while the accuracy in validation and test is image wise(i.e. the most common class for the patches of the same image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset,name):\n",
    "    \n",
    "    losses_ae = []\n",
    "    losses_c = []\n",
    "    count_correct = 0\n",
    "    count_input = 0\n",
    "    \n",
    "    real_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "\n",
    "    autoencoder.eval()\n",
    "    \n",
    "    #single image\n",
    "    for data in tqdm(dataset,desc = name +\" set\"):\n",
    "        pred = []\n",
    "        labels = torch.tensor(data[1]).to(device)\n",
    "        \n",
    "        #iterate over all the patches\n",
    "        for patch in data[0]:\n",
    "            \n",
    "            img = torch.tensor(patch).to(device)\n",
    "            \n",
    "            rec_img,pred_labels= autoencoder(img)  \n",
    "            \n",
    "            classifier_loss = criterion_classifier(pred_labels, labels)\n",
    "            ae_loss= criterion_ae(img,rec_img)\n",
    "            \n",
    "\n",
    "            losses_ae.append(ae_loss.detach().cpu().numpy())\n",
    "            losses_c.append(classifier_loss.detach().cpu().numpy())\n",
    "            \n",
    "            pred.append(pred_labels.to(\"cpu\").detach().numpy().argmax(axis=1)[0])\n",
    "            \n",
    "            \n",
    "        count_input += 1\n",
    "            \n",
    "        final_image_label = max(set(pred), key=pred.count) \n",
    "        true_image_label = labels.to(\"cpu\").detach().numpy().argmax(axis=1)[0]\n",
    "        count_correct+= (final_image_label == true_image_label)\n",
    "        \n",
    "        del img\n",
    "        del labels\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect();\n",
    "        \n",
    "        predicted_labels.append(final_image_label)\n",
    "        real_labels.append(true_image_label)\n",
    "        \n",
    "\n",
    "    # Print loss at end of each epoch\n",
    "    accuracy= (count_correct/count_input)*100.\n",
    "    print(name+': Autoencoder Loss: %.4f, Classifier Loss: %.4f, Accuracy: %.4f in %d input samples\\n'\n",
    "      % (np.mean(losses_ae), np.mean(losses_c),accuracy,count_input))\n",
    "    \n",
    "    return np.mean(losses_ae),np.mean(losses_c),accuracy,real_labels,predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17514270",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_classifier = nn.CrossEntropyLoss()#weight = torch.tensor(weights).to(device())\n",
    "criterion_ae = nn.MSELoss()\n",
    "\n",
    "def train(train_dataloader,val_dataset):\n",
    "    optimizer_ae = optim.RMSprop(autoencoder.parameters(), lr=learning_rate,weight_decay=1e-2)\n",
    "\n",
    "    \n",
    "    \n",
    "    best_epoch = 0 #(based on the validation accuracy)\n",
    "    best_accuracy = -np.inf\n",
    "    \n",
    "    #for plotting\n",
    "    train_loss_ae_trend = []\n",
    "    train_loss_c_trend = []\n",
    "    train_accuracy_trend =[]\n",
    "    val_loss_ae_trend = []\n",
    "    val_loss_c_trend = []\n",
    "    val_accuracy_trend = []\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        ######training#####\n",
    "        losses_ae = []\n",
    "        losses_c = []\n",
    "        count_correct = 0\n",
    "        count_input = 0\n",
    "\n",
    "        autoencoder.train()\n",
    "       \n",
    "        \n",
    "        #train classifier for one epoch    \n",
    "        for data in tqdm(train_dataloader,desc = \"TRAIN Epoch [%d/%d]\"%(epoch+1,num_epochs)):\n",
    "\n",
    "            img, labels = data\n",
    "            img = img.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "        \n",
    "            optimizer_ae.zero_grad()\n",
    "            \n",
    "            \n",
    "            rec_img,pred_labels= autoencoder(img)  \n",
    "            \n",
    "\n",
    "            classifier_loss = criterion_classifier(pred_labels, labels)\n",
    "            ae_loss = criterion_ae(img,rec_img)\n",
    "            total_loss = classifier_loss+50*ae_loss\n",
    "            \n",
    "            total_loss.backward()\n",
    "            \n",
    "            optimizer_ae.step()\n",
    "            \n",
    "            losses_ae.append(ae_loss.detach().cpu().numpy())\n",
    "            losses_c.append(classifier_loss.detach().cpu().numpy())\n",
    "\n",
    "            \n",
    "            count_input+=pred_labels.detach().cpu().numpy().shape[0]\n",
    "            count_correct+=np.sum((labels.to(\"cpu\").detach().numpy().argmax(axis=1)==pred_labels.to(\"cpu\").detach().numpy().argmax(axis=1)))\n",
    "            del img\n",
    "            del labels\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect();\n",
    "            \n",
    "        accuracy = (count_correct/count_input)*100.\n",
    "        \n",
    "        print('TRAIN: Epoch [%d/%d], Autoencoder Loss: %.4f, Classifier Loss: %.4f, Accuracy: %.4f in %d input patches \\n'\n",
    "              % (epoch+1, num_epochs, np.mean(losses_ae), np.mean(losses_c),accuracy,count_input))\n",
    "        \n",
    "        train_loss_ae_trend.append(np.mean(losses_ae))\n",
    "        train_loss_c_trend.append(np.mean(losses_c))\n",
    "        train_accuracy_trend.append(accuracy)\n",
    "        \n",
    "        ######validation######\n",
    "        ae,c,acc,_,_ = test(val_dataset,\"VALIDATION\")\n",
    "        \n",
    "        val_loss_ae_trend.append(ae)\n",
    "        val_loss_c_trend.append(c)\n",
    "        val_accuracy_trend.append(acc)\n",
    "        \n",
    "        if acc > best_accuracy:\n",
    "            print(GREEN+\"Saved Model: \"+END+UNDERLINE+\"best validation accuracy reached\\n\\n\"+END)\n",
    "            best_epoch = epoch+1\n",
    "            torch.save(autoencoder.state_dict(), os.path.join(models_dir,\"best_ae.pt\"))\n",
    "            \n",
    "            best_accuracy = acc\n",
    "        \n",
    "\n",
    "    return np.array([[train_loss_ae_trend,train_loss_c_trend,train_accuracy_trend],\n",
    "            [val_loss_ae_trend,val_loss_c_trend,val_accuracy_trend],best_epoch],dtype=np.object_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59edc2ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training or loading the results of a pre-trained model\n",
    "if(not LOAD_MODEL):\n",
    "    \n",
    "    results = train(train_dataloader,val_dataset) \n",
    "    np.save(os.path.join(models_dir,\"results.npy\"),results)\n",
    "    \n",
    "else: \n",
    "    results = np.load(os.path.join(models_dir,\"results.npy\"),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#always load the best model after the training\n",
    "autoencoder =  Autoencoder().to(device)\n",
    "autoencoder.load_state_dict(torch.load(os.path.join(models_dir,\"best_ae.pt\")))\n",
    "autoencoder.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377ae02",
   "metadata": {},
   "source": [
    "### Plot of the losses and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a5763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "train_ae = results[0][0] #mse\n",
    "train_c = results[0][1] #cross entropy\n",
    "train_acc = results[0][2] #accuracy\n",
    "\n",
    "#validation\n",
    "val_ae = results[1][0] #mse\n",
    "val_c = results[1][1] #cross entropy\n",
    "val_acc = results[1][2] #accuracy\n",
    "\n",
    "#best epoch number\n",
    "best_epoch = results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc51dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating the batch indexes\n",
    "batchID = np.array([i+1 for i in range(len(train_ae))])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "#plotting the losses\n",
    "plt.plot(batchID,train_ae,label = \"Training loss autoencoder\")\n",
    "plt.plot(batchID,val_ae, label = \"Validation loss autoencoder\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "#showing the position of the best values\n",
    "plt.axvline(x = best_epoch, color = 'b', label = \"Max validation accuracy\")\n",
    "\n",
    "#adding the position of the best values in the x axis\n",
    "ticks = list(plt.xticks()[0]) \n",
    "ticks.extend([best_epoch])\n",
    "\n",
    "#limit the plot x-values\n",
    "plt.xticks(ticks)\n",
    "\n",
    "plt.xlim(0,batchID.max()+1)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "#saving the figure\n",
    "#plt.savefig(\"training-val_mse\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f27214",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "#plotting the losses\n",
    "plt.plot(batchID,train_c,label = \"Training loss classifier\")\n",
    "plt.plot(batchID,val_c, label = \"Validation loss classifier\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy\")\n",
    "\n",
    "#showing the position of the best values\n",
    "plt.axvline(x = best_epoch, color = 'b', label = \"Max validation accuracy\")\n",
    "\n",
    "#adding the position of the best values in the x axis\n",
    "ticks = list(plt.xticks()[0]) \n",
    "ticks.extend([best_epoch])\n",
    "plt.xticks(ticks)\n",
    "\n",
    "#limit the plot x-values\n",
    "plt.xlim(0,batchID.max()+1)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "#saving the figure\n",
    "#plt.savefig(\"train-val_ce\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "print(len(batchID),len(train_c+50*np.ones(len(batchID))*train_ae))\n",
    "\n",
    "#plotting the losses\n",
    "plt.plot(batchID,train_c+50*np.ones(len(batchID))*train_ae,label = \"Training total loss\")\n",
    "plt.plot(batchID,val_c+50*np.ones(len(batchID))*val_ae, label = \"Validation total loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy + 50xMSE\")\n",
    "\n",
    "#showing the position of the best values\n",
    "plt.axvline(x = best_epoch, color = 'b', label = \"Max validation accuracy\")\n",
    "\n",
    "#adding the position of the best values in the x axis\n",
    "ticks = list(plt.xticks()[0]) \n",
    "ticks.extend([best_epoch])\n",
    "plt.xticks(ticks)\n",
    "\n",
    "#limit the plot x-values\n",
    "plt.xlim(0,batchID.max()+1)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "#saving the figure\n",
    "#plt.savefig(\"train-val_totalloss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "#plotting the accuracy\n",
    "plt.plot(batchID,train_acc,label = \"Training accuracy\")\n",
    "plt.plot(batchID,val_acc, label = \"Validation accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "#showing the position of the best values\n",
    "plt.axvline(x = best_epoch, color = 'b', label = \"Max validation accuracy\")\n",
    "\n",
    "#adding the position of the best values in the x axis\n",
    "ticks = list(plt.xticks()[0]) \n",
    "ticks.extend([best_epoch])\n",
    "plt.xticks(ticks)\n",
    "\n",
    "#limit the plot x-values\n",
    "plt.xlim(0,batchID.max()+1)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "#saving the figure\n",
    "#plt.savefig(\"train-val_accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dcbf59",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17494b39",
   "metadata": {},
   "source": [
    "Notice that the value of the losses and the accuracy for the train set are computed during the training, so they are not very accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1917d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix_pandas(labels,predictions,title,save_img = False):\n",
    "    \n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=labels, y_pred=predictions)\n",
    "    # Plot the confusion matrix as an image.\n",
    "    class_names = name_to_id.keys()\n",
    "    df_cm = pd.DataFrame(cm, index = class_names, columns = class_names)\n",
    "    plt.figure(figsize = (15,10))\n",
    "    sn.heatmap(df_cm, annot=True, cmap='Blues')\n",
    "    #plt.axis([-0.5, 13.5, 13.5, -0.5])\n",
    "    plt.title(title,fontsize=20)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    if save_img:\n",
    "        plt.savefig(\"./\"+title+\".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4bfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(data_dir,\"train\",\"images\")#images\n",
    "train_dir_patches = os.path.join(data_dir,\"train\",\"images_patch\")#patches\n",
    "train_labels_dir = os.path.join(data_dir,\"train\",\"labels\",\"train.json\")\n",
    "with open(train_labels_dir) as f:\n",
    "    train_labels = json.load(f)#labels\n",
    "\n",
    "train_dataset = HDAdatasetPatches(train_dir,train_dir_patches,train_labels,train = False,\n",
    "                          grayscale=grayscale,fft=fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641cfb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,real_train,pred_train = test(train_dataset,\"TRAIN\")\n",
    "_,_,_,real_val,pred_val = test(val_dataset,\"VALIDATION\")\n",
    "_,_,_,real_test,pred_test = test(test_dataset,\"TEST\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962ac6b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_confusion_matrix_pandas(real_train,pred_train,\"Train set\",save_img = False)\n",
    "print_confusion_matrix_pandas(real_val,pred_val,\"Validation set\",save_img = False)\n",
    "print_confusion_matrix_pandas(real_test,pred_test,\"Test set\",save_img = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5faa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
