{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37b31e6",
   "metadata": {},
   "source": [
    "## Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41ab6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import gc\n",
    "from tqdm import tqdm,trange\n",
    "\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "329a92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usefull for better outputs visualization\n",
    "PURPLE = \"\\033[95m\"\n",
    "CYAN = \"\\033[96m\"\n",
    "DARKCYAN = \"\\033[36m\"\n",
    "BLUE = \"\\033[94m\"\n",
    "GREEN = \"\\033[92m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "RED = \"\\033[91m\"\n",
    "BOLD = \"\\033[1m\"\n",
    "UNDERLINE = \"\\033[4m\"\n",
    "END = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7564cc00",
   "metadata": {},
   "source": [
    "### Project settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b42f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_unsplitted = \"dataset/unsplitted\"\n",
    "save_dir_splitted = \"dataset/splitted\"\n",
    "data_dir = \"dataset/splitted\"\n",
    "models_dir =\"models/\" #the resuls will be saved here\n",
    "\n",
    "os.makedirs(models_dir,exist_ok=True)\n",
    "\n",
    "\n",
    "patch_size = 64\n",
    "n_samples = 40\n",
    "batch_size = 75\n",
    "\n",
    "\n",
    "grayscale= False\n",
    "resize_to= None\n",
    "fft = False\n",
    "\n",
    "\n",
    "learning_rate= 0.001\n",
    "num_epochs = 50\n",
    "\n",
    "\n",
    "#@DEPRECATED VARIABLE\n",
    "#ALREADY_SPLITTED = True #-> we will use only \"LOAD_DATASET\"\n",
    "\n",
    "\n",
    "LOAD_DATASET = True\n",
    "\n",
    "#to be implemented:\n",
    "\n",
    "DATASET_ALREADY_DOWNLOADED = True \n",
    "DATASET_ALREADY_UNZIPPED = True\n",
    "\n",
    "LOAD_MODEL = False \n",
    "MODEL_ALREADY_DOWNLOADED = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b6f851",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1385d",
   "metadata": {},
   "source": [
    "### Dataset splitting and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a30151b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLL: 100%|███████████████████████████████████████████████████████████████████████████| 113/113 [00:01<00:00, 72.06it/s]\n",
      "FL: 100%|████████████████████████████████████████████████████████████████████████████| 139/139 [00:02<00:00, 63.02it/s]\n",
      "MCL: 100%|██████████████████████████████████████████████████████████████████████████| 122/122 [00:01<00:00, 103.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 261\n",
      "Test samples: 75\n",
      "Validation samples: 38\n"
     ]
    }
   ],
   "source": [
    "classes = [\"CLL\", \"FL\", \"MCL\"]\n",
    "id_to_name = {0:\"CLL\",1:\"FL\",2:\"MCL\"}\n",
    "\n",
    "if(not LOAD_DATASET):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i, cl in enumerate(classes):\n",
    "        path = os.path.join(data_dir_unsplitted, cl)\n",
    "        images = os.listdir(path)\n",
    "        for img in tqdm(images,desc=cl):\n",
    "            img_path = os.path.join(path, img)\n",
    "            img = cv2.imread(img_path).astype(np.uint8)\n",
    "            \n",
    "            data.append(img)\n",
    "            labels.append(i) \n",
    "\n",
    "\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.125, random_state=42)\n",
    "\n",
    "    print(\"Train samples:\", len(train_data))\n",
    "    print(\"Test samples:\", len(test_data))\n",
    "    print(\"Validation samples:\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17f646",
   "metadata": {},
   "source": [
    "### Saving the splitted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90a42d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, patch_size, n_samples = 40):\n",
    "    h, w, _ = image.shape\n",
    "    h_eff = h - patch_size\n",
    "    w_eff = w - patch_size\n",
    "    samples = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        h_vertex = np.random.randint(h_eff)\n",
    "        w_vertex = np.random.randint(w_eff)\n",
    "        samples.append(image[h_vertex:h_vertex+patch_size, w_vertex:w_vertex+patch_size, :])\n",
    "    \n",
    "    samples = np.stack(samples)\n",
    "    return samples   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd4d2b3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|█████████████████████████████████████████████████████████████████████████| 261/261 [00:22<00:00, 11.65it/s]\n",
      "Test: 100%|████████████████████████████████████████████████████████████████████████████| 75/75 [00:08<00:00,  8.98it/s]\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 38/38 [00:04<00:00,  8.70it/s]\n"
     ]
    }
   ],
   "source": [
    "#SAVING THE DATASET\n",
    "if(not LOAD_DATASET):\n",
    "    #TRAIN\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"train\",\"images\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"train\",\"images_patch\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"train\",\"labels\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"train\",\"labels_patch\"),exist_ok=True)\n",
    "     \n",
    "    train_labels_dict={}\n",
    "    train_labels_patch_dict={}\n",
    "    \n",
    "    for i, image in enumerate(tqdm(train_data,desc = \"Train\")):                       #images\n",
    "        file_name = str(i)+\"_train\"+\".png\"\n",
    "        \n",
    "        path = os.path.join(save_dir_splitted,\"train\",\"images\",file_name)\n",
    "        train_labels_dict[file_name] = int(train_labels[i])\n",
    "        cv2.imwrite(path,image.astype(np.float64))\n",
    "        \n",
    "        image_patch = crop_image(image, patch_size, n_samples)\n",
    "        for j in range(len(image_patch)):\n",
    "            file_name = str(i)+\"-\"+str(j)+\"_train\"+\".png\"\n",
    "            path = os.path.join(save_dir_splitted,\"train\",\"images_patch\",file_name)\n",
    "            train_labels_patch_dict[file_name] = int(train_labels[i])\n",
    "            cv2.imwrite(path,image_patch[j].astype(np.float64))\n",
    "       \n",
    "    \n",
    "    \n",
    "    with open(os.path.join(save_dir_splitted,\"train\",\"labels\",\"train.json\"), 'w') as f:\n",
    "        json.dump(train_labels_dict, f)#labels\n",
    "    with open(os.path.join(save_dir_splitted,\"train\",\"labels_patch\",\"train_patch.json\"), 'w') as f:\n",
    "        json.dump(train_labels_patch_dict, f)#labels\n",
    "\n",
    "    #TEST\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"test\",\"images\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"test\",\"images_patch\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"test\",\"labels\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"test\",\"labels_patch\"),exist_ok=True)\n",
    "\n",
    "    test_labels_dict={}\n",
    "    test_labels_patch_dict={}\n",
    "    \n",
    "    for i, image in enumerate(tqdm(test_data,desc = \"Test\")):                      #images\n",
    "        file_name = str(i)+\"_test\"+\".png\"\n",
    "        path = os.path.join(save_dir_splitted,\"test\",\"images\",file_name)\n",
    "        \n",
    "        cv2.imwrite(path,image.astype(np.float64))\n",
    "        test_labels_dict[file_name] = int(test_labels[i]) \n",
    "        \n",
    "        image_patch = crop_image(image, patch_size, 80)\n",
    "        for j in range(len(image_patch)):\n",
    "            file_name = str(i)+\"-\"+str(j)+\"_test\"+\".png\"\n",
    "            path = os.path.join(save_dir_splitted,\"test\",\"images_patch\",file_name)\n",
    "            test_labels_patch_dict[file_name] = int(test_labels[i])\n",
    "            cv2.imwrite(path,image_patch[j].astype(np.float64))\n",
    "\n",
    "    with open(os.path.join(save_dir_splitted,\"test\",\"labels\",\"test.json\"), 'w') as f:\n",
    "        json.dump(test_labels_dict, f)#labels\n",
    "    with open(os.path.join(save_dir_splitted,\"test\",\"labels_patch\",\"test_patch.json\"), 'w') as f:\n",
    "        json.dump(test_labels_patch_dict, f)#labels\n",
    "        \n",
    "    #VALIDATION\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"val\",\"images\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"val\",\"images_patch\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"val\",\"labels\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"val\",\"labels_patch\"),exist_ok=True)\n",
    "    \n",
    "    val_labels_dict = {}\n",
    "    val_labels_patch_dict = {}\n",
    "    \n",
    "    for i, image in enumerate(tqdm(val_data,desc = \"Validation\")):               #images\n",
    "        file_name = str(i)+\"_val\"+\".png\"\n",
    "        path = os.path.join(save_dir_splitted,\"val\",\"images\",file_name)\n",
    "        \n",
    "        cv2.imwrite(path,image.astype(np.float64))\n",
    "        val_labels_dict[file_name] = int(val_labels[i])\n",
    "        \n",
    "        image_patch = crop_image(image, patch_size, 80)\n",
    "        for j in range(len(image_patch)):\n",
    "            file_name = str(i)+\"-\"+str(j)+\"_val\"+\".png\"\n",
    "            path = os.path.join(save_dir_splitted,\"val\",\"images_patch\",file_name)\n",
    "            val_labels_patch_dict[file_name] = int(val_labels[i])\n",
    "            cv2.imwrite(path,image_patch[j].astype(np.float64))\n",
    "        \n",
    "    with open(os.path.join(save_dir_splitted,\"val\",\"labels\",\"val.json\"), 'w') as f:\n",
    "        json.dump(val_labels_dict, f)#labels\n",
    "    with open(os.path.join(save_dir_splitted,\"val\",\"labels_patch\",\"val_patch.json\"), 'w') as f:\n",
    "        json.dump(val_labels_patch_dict, f)#labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab21346c",
   "metadata": {},
   "source": [
    "### Dataloaders definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a1b860",
   "metadata": {},
   "source": [
    "Notice that the labels are saved into a numpy array, while the images are read directly from the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9cb6438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDAdataset(Dataset):\n",
    "    def __init__(self, root_dir, labels, train = False, transform=None, grayscale= False, resize_to= None, fft = False):\n",
    "        self.root_dir = root_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.grayscale=grayscale\n",
    "        self.fft = fft\n",
    "        self.resize_to = resize_to\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.root_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.listdir(self.root_dir)[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        \n",
    "        if self.grayscale or self.fft:\n",
    "            image = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "        else:\n",
    "            image = cv2.imread(img_path)\n",
    "\n",
    "        \n",
    "        if self.train:\n",
    "            image = image.astype(np.uint8) + np.random.normal(0, 2, image.shape).astype(np.uint8)\n",
    "        \n",
    "        if self.resize_to is not None:\n",
    "            image = cv2.resize(image, resize_to, interpolation= cv2.INTER_LINEAR)\n",
    "            \n",
    "        if self.fft:\n",
    "            image = np.fft.fft2(image)\n",
    "            fshift = np.fft.fftshift(image)\n",
    "            magnitude_spectrum = 20*np.log(np.abs(fshift))\n",
    "            image = magnitude_spectrum\n",
    "        \n",
    "\n",
    "    \n",
    "        #labels smoothing\n",
    "        label = 0.1*np.ones(3)\n",
    "        label[self.labels[img_name]] = 0.8\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform((image/255.).astype(np.float32))\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ce0dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDAdatasetPatches(Dataset):\n",
    "    def __init__(self, root_dir,patch_dir, labels, train = False, grayscale= False, fft = False):\n",
    "        self.root_dir = root_dir\n",
    "        self.patch_dir = patch_dir\n",
    "        self.labels = labels\n",
    "        self.train = train\n",
    "        self.grayscale=grayscale\n",
    "        self.fft = fft\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.root_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.listdir(self.root_dir)[idx]\n",
    "        img_id = int(img_name.split(\"_\")[0])\n",
    "        \n",
    "        patches_list = os.listdir(self.patch_dir)\n",
    "        patches_image_list = list(filter(lambda name: int(name.split(\"-\")[0]) == img_id, patches_list))\n",
    "        \n",
    "        all_patches = []\n",
    "        for patch in patches_image_list:\n",
    "            patch_path = os.path.join(self.patch_dir, patch)\n",
    "            \n",
    "\n",
    "            if self.grayscale or self.fft:\n",
    "                image = cv2.imread(patch_path,cv2.IMREAD_GRAYSCALE)\n",
    "                channels = 1\n",
    "            \n",
    "            else:\n",
    "                image = cv2.imread(patch_path)\n",
    "                channels = 3\n",
    "\n",
    "            if self.train:\n",
    "                image = image.astype(np.uint8) + np.random.normal(0, 5, image.shape).astype(np.uint8)\n",
    "\n",
    "            if self.fft:\n",
    "                image = np.fft.fft2(image)\n",
    "                fshift = np.fft.fftshift(image)\n",
    "                magnitude_spectrum = 20*np.log(np.abs(fshift))\n",
    "                image = magnitude_spectrum\n",
    "            \n",
    "            image = (image/255.).astype(np.float32)\n",
    "            image = image.reshape(image.shape[0],image.shape[1],channels)\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "            all_patches.append(image.reshape(1,image.shape[0],image.shape[1],image.shape[2]))\n",
    "\n",
    "        all_patches=np.stack(all_patches)\n",
    "        \n",
    "        #labels smoothing\n",
    "        label = 0.1*np.ones(3)\n",
    "        label[self.labels[img_name]] = 0.8\n",
    "\n",
    "        return all_patches, label.reshape(1,label.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26887536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating the train set list: 100%|█████████████████████████████████████████████| 10440/10440 [01:56<00:00, 89.82it/s]\n"
     ]
    }
   ],
   "source": [
    "if(LOAD_DATASET and not DATASET_ALREADY_DOWNLOADED):\n",
    "    \n",
    "    print(\"download and extract the datasets\")\n",
    "    urllib.request.urlretrieve(\"\", \"dataset.zip\")\n",
    "    \n",
    "    with zipfile.ZipFile(\"dataset.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./dataset/\")\n",
    "\n",
    "        \n",
    "#convert to tensor each image\n",
    "transform = transforms.Compose([transforms.ToTensor()])    \n",
    "\n",
    "\n",
    "#TRAIN DATASET AND DATALOADER\n",
    "train_dir = os.path.join(data_dir,\"train\",\"images_patch\")#images\n",
    "train_labels_dir = os.path.join(data_dir,\"train\",\"labels_patch\",\"train_patch.json\")\n",
    "with open(train_labels_dir) as f:\n",
    "    train_labels = json.load(f)#labels\n",
    "\n",
    "train_dataset = HDAdataset(train_dir,train_labels,train = True, transform=transform\n",
    "                           ,grayscale=grayscale,fft=fft)\n",
    "\n",
    "#using a list to increase the training speed since the data fit in the ram\n",
    "train_data = [data for data in tqdm(train_dataset,desc=\"Generating the train set list\")] \n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "#TEST DATASET\n",
    "test_dir = os.path.join(data_dir,\"test\",\"images\")#images\n",
    "test_dir_patches = os.path.join(data_dir,\"test\",\"images_patch\")#patches\n",
    "test_labels_dir = os.path.join(data_dir,\"test\",\"labels\",\"test.json\")\n",
    "with open(test_labels_dir) as f:\n",
    "    test_labels = json.load(f)#labels\n",
    "\n",
    "test_dataset = HDAdatasetPatches(test_dir,test_dir_patches,test_labels,train = False,\n",
    "                          grayscale=grayscale,fft=fft)\n",
    "\n",
    "\n",
    "#VALIDATION DATASET\n",
    "val_dir = os.path.join(data_dir,\"val\",\"images\")#images\n",
    "val_dir_patches = os.path.join(data_dir,\"val\",\"images_patch\")#patches\n",
    "val_labels_dir = os.path.join(data_dir,\"val\",\"labels\",\"val.json\")\n",
    "with open(val_labels_dir) as f:\n",
    "    val_labels = json.load(f)#labels\n",
    "\n",
    "val_dataset = HDAdatasetPatches(val_dir,val_dir_patches,val_labels,train = False,\n",
    "                          grayscale=grayscale,fft=fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90478310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weight(dataset):\n",
    "    count = {0:0,1:0,2:0}\n",
    "    for data in tqdm(dataset,desc = \"Computing class weight\"):\n",
    "        label = data[1]\n",
    "        count[label.argmax()]= count[label.argmax()]+1\n",
    "    \n",
    "    count = np.array([count[0],count[1],count[2]])\n",
    "    total = count.sum()\n",
    "    return count/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "00b2bebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing class weight: 100%|████████████████████████████████████████████████| 10440/10440 [00:00<00:00, 417658.15it/s]\n"
     ]
    }
   ],
   "source": [
    "weights = compute_weight(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "effa2bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26819923, 0.38314176, 0.348659  ])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5599d",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "915f464f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684cf15",
   "metadata": {},
   "source": [
    "### Models definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f89e0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        #encoder\n",
    "        self.convE1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3,3), stride=(2,2), padding=(0,0))\n",
    "        self.convE2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3,3), stride=(2,2), padding=(0,0))\n",
    "        self.convE3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), stride=(2,2), padding=(0,0))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.BNE16 = nn.BatchNorm2d(16)\n",
    "        self.BNE32 = nn.BatchNorm2d(32)\n",
    "        self.BNE64 = nn.BatchNorm2d(64)\n",
    "        self.linearE1 = nn.Linear(3136,20)\n",
    "        self.linearE2 = nn.Linear(20,3)\n",
    "        self.BNE150linear = nn.BatchNorm1d(20)\n",
    "        \n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        \n",
    "        #decoder\n",
    "        \n",
    "        self.convD1 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=(3,3), stride=(2,2), padding=(0,0))\n",
    "        self.convD2 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=(3,3), stride=(2,2), padding=(0,0))\n",
    "        self.convD3 = nn.ConvTranspose2d(in_channels=16, out_channels=3, kernel_size=(3,3), \n",
    "                                         stride=(2,2), padding=(0,0), dilation=1,output_padding=(1,1))\n",
    "        self.reshape = nn.Unflatten(-1, (64,7,7))\n",
    "        self.BND16 = nn.BatchNorm2d(16)\n",
    "        self.BND32 = nn.BatchNorm2d(32)\n",
    "        self.linearD1 = nn.Linear(3,20)\n",
    "        self.linearD2 = nn.Linear(20,3136)\n",
    "        self.BND150linear = nn.BatchNorm1d(20)\n",
    "        self.BND16klinear = nn.BatchNorm1d(3136)\n",
    "        \n",
    "        self.out = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x1 = self.act(self.BNE16(self.convE1(x)))\n",
    "        x2 = self.act(self.BNE32(self.convE2(x1)))\n",
    "        x3 = self.act(self.BNE64(self.convE3(x2)))\n",
    "        x3linear = self.flatten(x3)\n",
    "        x4 = self.drop(self.act(self.BNE150linear(self.linearE1(x3linear))))\n",
    "        x5 = self.linearE2(x4)\n",
    "\n",
    "        x = self.act(self.BND150linear(self.linearD1(x5)))\n",
    "        x = self.act(self.BND16klinear(self.linearD2(x+x4)))\n",
    "        x = self.reshape(x)\n",
    "        x = self.act(self.BND32(self.convD1(x+x3)))\n",
    "        x = self.act(self.BND16(self.convD2(x+x2)))\n",
    "        x = self.out(self.convD3(x+x1))\n",
    "\n",
    "        return x,self.softmax(x5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f2c567e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mYou are working on: \u001b[0mcuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(BOLD+\"You are working on: \"+END+str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f5a37",
   "metadata": {},
   "source": [
    "### Loading of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9139a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(LOAD_MODEL and not MODEL_ALREADY_DOWNLOADED):\n",
    "    print(\"download the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c6460ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not LOAD_MODEL):\n",
    "    autoencoder = Autoencoder().to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d1214cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [75, 16, 31, 31]             448\n",
      "       BatchNorm2d-2           [75, 16, 31, 31]              32\n",
      "         LeakyReLU-3           [75, 16, 31, 31]               0\n",
      "            Conv2d-4           [75, 32, 15, 15]           4,640\n",
      "       BatchNorm2d-5           [75, 32, 15, 15]              64\n",
      "         LeakyReLU-6           [75, 32, 15, 15]               0\n",
      "            Conv2d-7             [75, 64, 7, 7]          18,496\n",
      "       BatchNorm2d-8             [75, 64, 7, 7]             128\n",
      "         LeakyReLU-9             [75, 64, 7, 7]               0\n",
      "          Flatten-10                 [75, 3136]               0\n",
      "           Linear-11                   [75, 20]          62,740\n",
      "      BatchNorm1d-12                   [75, 20]              40\n",
      "        LeakyReLU-13                   [75, 20]               0\n",
      "          Dropout-14                   [75, 20]               0\n",
      "           Linear-15                    [75, 3]              63\n",
      "           Linear-16                   [75, 20]              80\n",
      "      BatchNorm1d-17                   [75, 20]              40\n",
      "        LeakyReLU-18                   [75, 20]               0\n",
      "           Linear-19                 [75, 3136]          65,856\n",
      "      BatchNorm1d-20                 [75, 3136]           6,272\n",
      "        LeakyReLU-21                 [75, 3136]               0\n",
      "        Unflatten-22             [75, 64, 7, 7]               0\n",
      "  ConvTranspose2d-23           [75, 32, 15, 15]          18,464\n",
      "      BatchNorm2d-24           [75, 32, 15, 15]              64\n",
      "        LeakyReLU-25           [75, 32, 15, 15]               0\n",
      "  ConvTranspose2d-26           [75, 16, 31, 31]           4,624\n",
      "      BatchNorm2d-27           [75, 16, 31, 31]              32\n",
      "        LeakyReLU-28           [75, 16, 31, 31]               0\n",
      "  ConvTranspose2d-29            [75, 3, 64, 64]             435\n",
      "          Sigmoid-30            [75, 3, 64, 64]               0\n",
      "          Softmax-31                    [75, 3]               0\n",
      "================================================================\n",
      "Total params: 182,518\n",
      "Trainable params: 182,518\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.52\n",
      "Forward/backward pass size (MB): 106.01\n",
      "Params size (MB): 0.70\n",
      "Estimated Total Size (MB): 110.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(autoencoder, (3, 64, 64), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931580de",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "The accuracy in the training is patch wise while the accuracy in validation and test is image wise(i.e. the most common class for the patches of the same image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9b91a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset,name):\n",
    "    \n",
    "    losses_ae = []\n",
    "    losses_c = []\n",
    "    count_correct = 0\n",
    "    count_input = 0\n",
    "\n",
    "    autoencoder.eval()\n",
    "    \n",
    "    #single image\n",
    "    for data in tqdm(dataset,desc = name +\" set\"):\n",
    "        pred = []\n",
    "        labels = torch.tensor(data[1]).to(device)\n",
    "        \n",
    "        #iterate over all the patches\n",
    "        for patch in data[0]:\n",
    "            \n",
    "            img = torch.tensor(patch).to(device)\n",
    "            \n",
    "            rec_img,pred_labels= autoencoder(img)  \n",
    "            \n",
    "            classifier_loss = criterion_classifier(pred_labels, labels)\n",
    "            ae_loss= criterion_ae(img,rec_img)\n",
    "            \n",
    "\n",
    "            losses_ae.append(ae_loss.detach().cpu().numpy())\n",
    "            losses_c.append(classifier_loss.detach().cpu().numpy())\n",
    "            \n",
    "            pred.append(pred_labels.to(\"cpu\").detach().numpy().argmax(axis=1)[0])\n",
    "            \n",
    "            \n",
    "        count_input += 1\n",
    "            \n",
    "        final_image_label = max(set(pred), key=pred.count) \n",
    "        true_image_label = labels.to(\"cpu\").detach().numpy().argmax(axis=1)[0]\n",
    "        count_correct+= (final_image_label == true_image_label)\n",
    "        \n",
    "        del img\n",
    "        del labels\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect();\n",
    "\n",
    "    # Print loss at end of each epoch\n",
    "    accuracy= (count_correct/count_input)*100.\n",
    "    print(name+': Autoencoder Loss: %.4f, Classifier Loss: %.4f, Accuracy: %.4f in %d input samples\\n'\n",
    "      % (np.mean(losses_ae), np.mean(losses_c),accuracy,count_input))\n",
    "    \n",
    "    return np.mean(losses_ae),np.mean(losses_c),accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "17514270",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_classifier = nn.CrossEntropyLoss(weight=torch.tensor(weights).to(device))\n",
    "criterion_ae = nn.MSELoss()\n",
    "\n",
    "def train(train_dataloader,val_dataset):\n",
    "    optimizer_ae = optim.RMSprop(autoencoder.parameters(), lr=learning_rate,weight_decay=1e-2)\n",
    "\n",
    "    \n",
    "    \n",
    "    best_epoch = 0 #(based on the validation accuracy)\n",
    "    best_accuracy = -np.inf\n",
    "    \n",
    "    #for plotting\n",
    "    train_loss_ae_trend = []\n",
    "    train_loss_c_trend = []\n",
    "    train_accuracy_trend =[]\n",
    "    val_loss_ae_trend = []\n",
    "    val_loss_c_trend = []\n",
    "    val_accuracy_trend = []\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        ######training#####\n",
    "        losses_ae = []\n",
    "        losses_c = []\n",
    "        count_correct = 0\n",
    "        count_input = 0\n",
    "\n",
    "        autoencoder.train()\n",
    "       \n",
    "        \n",
    "        #train classifier for one epoch    \n",
    "        for data in tqdm(train_dataloader,desc = \"TRAIN Epoch [%d/%d]\"%(epoch+1,num_epochs)):\n",
    "\n",
    "            img, labels = data\n",
    "            img = img.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "        \n",
    "            optimizer_ae.zero_grad()\n",
    "            \n",
    "            \n",
    "            rec_img,pred_labels= autoencoder(img)  \n",
    "            \n",
    "\n",
    "            classifier_loss = criterion_classifier(pred_labels, labels)\n",
    "            ae_loss = criterion_ae(img,rec_img)\n",
    "            total_loss = classifier_loss+10*ae_loss\n",
    "            \n",
    "            total_loss.backward()\n",
    "            \n",
    "            optimizer_ae.step()\n",
    "            \n",
    "            losses_ae.append(ae_loss.detach().cpu().numpy())\n",
    "            losses_c.append(classifier_loss.detach().cpu().numpy())\n",
    "\n",
    "            \n",
    "            count_input+=pred_labels.detach().cpu().numpy().shape[0]\n",
    "            count_correct+=np.sum((labels.to(\"cpu\").detach().numpy().argmax(axis=1)==pred_labels.to(\"cpu\").detach().numpy().argmax(axis=1)))\n",
    "            del img\n",
    "            del labels\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect();\n",
    "            \n",
    "        accuracy = (count_correct/count_input)*100.\n",
    "        \n",
    "        print('TRAIN: Epoch [%d/%d], Autoencoder Loss: %.4f, Classifier Loss: %.4f, Accuracy: %.4f in %d input patches \\n'\n",
    "              % (epoch+1, num_epochs, np.mean(losses_ae), np.mean(losses_c),accuracy,count_input))\n",
    "        \n",
    "        train_loss_ae_trend.append(np.mean(losses_ae))\n",
    "        train_loss_c_trend.append(np.mean(losses_c))\n",
    "        train_accuracy_trend.append(accuracy)\n",
    "        \n",
    "        ######validation######\n",
    "        ae,c,acc = test(val_dataset,\"VALIDATION\")\n",
    "        \n",
    "        val_loss_ae_trend.append(ae)\n",
    "        val_loss_c_trend.append(c)\n",
    "        val_accuracy_trend.append(acc)\n",
    "        \n",
    "        if acc > best_accuracy:\n",
    "            print(GREEN+\"Saved Model: \"+END+UNDERLINE+\"best validation accuracy reached\\n\\n\"+END)\n",
    "            best_epoch = epoch+1\n",
    "            torch.save(autoencoder.state_dict(), os.path.join(models_dir,\"best_ae.pt\"))\n",
    "            \n",
    "            best_accuracy = acc\n",
    "        \n",
    "\n",
    "    return np.array([[train_loss_ae_trend,train_loss_c_trend,train_accuracy_trend],\n",
    "            [val_loss_ae_trend,val_loss_c_trend,val_accuracy_trend],best_epoch],dtype=np.object_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59edc2ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN Epoch [1/50]: 100%|████████████████████████████████████████████████████████████| 139/139 [00:14<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Epoch [1/50], Autoencoder Loss: 0.0050, Classifier Loss: 0.3433, Accuracy: 55.1463 in 10425 input patches \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALIDATION set: 100%|██████████████████████████████████████████████████████████████████| 38/38 [00:11<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION: Autoencoder Loss: 0.0053, Classifier Loss: 0.3632, Accuracy: 44.7368 in 38 input samples\n",
      "\n",
      "\u001b[92mSaved Model: \u001b[0m\u001b[4mbest validation accuracy reached\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN Epoch [2/50]:  59%|███████████████████████████████████▉                         | 82/139 [00:08<00:07,  8.09it/s]"
     ]
    }
   ],
   "source": [
    "#training or loading the results of a pre-trained model\n",
    "if(not LOAD_MODEL):\n",
    "    \n",
    "    results = train(train_dataloader,val_dataset) \n",
    "    np.save(os.path.join(models_dir,\"results.npy\"),results)\n",
    "    \n",
    "else: \n",
    "    results = np.load(os.path.join(models_dir,\"results.npy\"),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96c6b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#always load the best model after the training\n",
    "autoencoder =  Autoencoder().to(device)\n",
    "autoencoder.load_state_dict(torch.load(os.path.join(models_dir,\"best_ae.pt\")))\n",
    "autoencoder.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377ae02",
   "metadata": {},
   "source": [
    "### Plot of the losses and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a5763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "train_ae = results[0][0] #mse\n",
    "train_c = results[0][1] #cross entropy\n",
    "train_acc = results[0][2] #accuracy\n",
    "\n",
    "#validation\n",
    "val_ae = results[1][0] #mse\n",
    "val_c = results[1][1] #cross entropy\n",
    "val_acc = results[1][2] #accuracy\n",
    "\n",
    "#best epoch number\n",
    "best_epoch = results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc51dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating the batch indexes\n",
    "batchID = np.array([i+1 for i in range(len(train_ae))])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "#plotting the losses\n",
    "plt.plot(batchID,train_ae,label = \"Training loss autoencoder\")\n",
    "plt.plot(batchID,val_ae, label = \"Validation loss autoencoder\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "#showing the position of the best values\n",
    "plt.axvline(x = best_epoch, color = 'b', label = \"Max validation accuracy\")\n",
    "\n",
    "#adding the position of the best values in the x axis\n",
    "ticks = list(plt.xticks()[0]) \n",
    "ticks.extend([best_epoch])\n",
    "\n",
    "#limit the plot x-values\n",
    "plt.xticks(ticks)\n",
    "\n",
    "plt.xlim(0,batchID.max()+1)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "#saving the figure\n",
    "#plt.savefig(\"training-val_mse\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f27214",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "#plotting the losses\n",
    "plt.plot(batchID,train_c,label = \"Training loss classifier\")\n",
    "plt.plot(batchID,val_c, label = \"Validation loss classifier\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy\")\n",
    "\n",
    "#showing the position of the best values\n",
    "plt.axvline(x = best_epoch, color = 'b', label = \"Max validation accuracy\")\n",
    "\n",
    "#adding the position of the best values in the x axis\n",
    "ticks = list(plt.xticks()[0]) \n",
    "ticks.extend([best_epoch])\n",
    "plt.xticks(ticks)\n",
    "\n",
    "#limit the plot x-values\n",
    "plt.xlim(0,batchID.max()+1)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "#saving the figure\n",
    "#plt.savefig(\"train-val_ce\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "#plotting the accuracy\n",
    "plt.plot(batchID,train_acc,label = \"Training accuracy\")\n",
    "plt.plot(batchID,val_acc, label = \"Validation accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "#showing the position of the best values\n",
    "plt.axvline(x = best_epoch, color = 'b', label = \"Max validation accuracy\")\n",
    "\n",
    "#adding the position of the best values in the x axis\n",
    "ticks = list(plt.xticks()[0]) \n",
    "ticks.extend([best_epoch])\n",
    "plt.xticks(ticks)\n",
    "\n",
    "#limit the plot x-values\n",
    "plt.xlim(0,batchID.max()+1)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "#saving the figure\n",
    "#plt.savefig(\"train-val_accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dcbf59",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17494b39",
   "metadata": {},
   "source": [
    "Notice that the value of the losses and the accuracy for the train set are computed during the training, so they are not very accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a4bfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(data_dir,\"train\",\"images\")#images\n",
    "train_dir_patches = os.path.join(data_dir,\"train\",\"images_patch\")#patches\n",
    "train_labels_dir = os.path.join(data_dir,\"train\",\"labels\",\"train.json\")\n",
    "with open(train_labels_dir) as f:\n",
    "    train_labels = json.load(f)#labels\n",
    "\n",
    "train_dataset = HDAdatasetPatches(train_dir,train_dir_patches,train_labels,train = False,\n",
    "                          grayscale=grayscale,fft=fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "641cfb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TEST set: 100%|████████████████████████████████████████████████████████████████████████| 75/75 [00:50<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Autoencoder Loss: 0.0019, Classifier Loss: 0.8800, Accuracy: 86.6667 in 75 input samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test(train_dataset,\"TRAIN\")\n",
    "test(val_dataset,\"VALIDATION\")\n",
    "test(test_dataset,\"TEST\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c684618b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ee072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
