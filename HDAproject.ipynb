{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ab6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import gc\n",
    "from tqdm import tqdm,trange\n",
    "\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329a92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usefull for better outputs visualization\n",
    "PURPLE = '\\033[95m'\n",
    "CYAN = '\\033[96m'\n",
    "DARKCYAN = '\\033[36m'\n",
    "BLUE = '\\033[94m'\n",
    "GREEN = '\\033[92m'\n",
    "YELLOW = '\\033[93m'\n",
    "RED = '\\033[91m'\n",
    "BOLD = '\\033[1m'\n",
    "UNDERLINE = '\\033[4m'\n",
    "END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b42f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_unsplitted = \"dataset/unsplitted\"\n",
    "save_dir_splitted = \"dataset/splitted\"\n",
    "data_dir = \"dataset/splitted\"\n",
    "\n",
    "ALREADY_SPLITTED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30151b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"CLL\", \"FL\", \"MCL\"]\n",
    "id_to_name = {0:\"CLL\",1:\"FL\",2:\"MCL\"}\n",
    "if(not ALREADY_SPLITTED):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i, cl in enumerate(classes):\n",
    "        path = os.path.join(data_dir_unsplitted, cl)\n",
    "        images = os.listdir(path)\n",
    "        for img in tqdm(images,desc=cl):\n",
    "            img_path = os.path.join(path, img)\n",
    "            img = cv2.imread(img_path).astype(np.uint8)\n",
    "\n",
    "            #DATA AUGMENTATION\n",
    "\n",
    "            #flipping the image\n",
    "            flipped_img = np.fliplr(img)\n",
    "            #gaussian noise\n",
    "            noise = np.random.normal(0, 5, img.shape).astype(np.uint8)\n",
    "            noisy_img = img + noise\n",
    "\n",
    "            #adding the data and the labels to the lists\n",
    "            data.append(flipped_img)\n",
    "            labels.append(i)\n",
    "            data.append(noisy_img)\n",
    "            labels.append(i)\n",
    "            data.append(img)\n",
    "            labels.append(i) \n",
    "\n",
    "\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.125, random_state=42)\n",
    "\n",
    "    print(\"Train samples:\", len(train_data))\n",
    "    print(\"Test samples:\", len(test_data))\n",
    "    print(\"Validation samples:\", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd4d2b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING THE DATASET\n",
    "if(not ALREADY_SPLITTED):\n",
    "    #TRAIN\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"train\",\"images\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"train\",\"labels\"),exist_ok=True)\n",
    "\n",
    "    np.save(os.path.join(save_dir_splitted,\"train\",\"labels\",\"train.npy\"),train_labels)#labels\n",
    "\n",
    "    for i, image in enumerate(tqdm(train_data,desc = \"Train\")):                       #images\n",
    "        file_name = str(i)+\"_train\"+\".png\"\n",
    "        path = os.path.join(save_dir_splitted,\"train\",\"images\",file_name)\n",
    "        cv2.imwrite(path,image.astype(np.float64))\n",
    "\n",
    "    #TEST\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"test\",\"images\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"test\",\"labels\"),exist_ok=True)\n",
    "\n",
    "    np.save(os.path.join(save_dir_splitted,\"test\",\"labels\",\"test.npy\"),test_labels)#labels\n",
    "\n",
    "    for i, image in enumerate(tqdm(test_data,desc = \"Test\")):                      #images\n",
    "        file_name = str(i)+\"_test\"+\".png\"\n",
    "        path = os.path.join(save_dir_splitted,\"test\",\"images\",file_name)\n",
    "        cv2.imwrite(path,image.astype(np.float64))\n",
    "\n",
    "    #VALIDATION\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"val\",\"images\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"val\",\"labels\"),exist_ok=True)\n",
    "\n",
    "    np.save(os.path.join(save_dir_splitted,\"val\",\"labels\",\"val.npy\"),val_labels) #labels\n",
    "\n",
    "    for i, image in enumerate(tqdm(val_data,desc = \"Validation\")):               #images\n",
    "        file_name = str(i)+\"_val\"+\".png\"\n",
    "        path = os.path.join(save_dir_splitted,\"val\",\"images\",file_name)\n",
    "        cv2.imwrite(path,image.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "915f464f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54470d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3,3), stride=(3,3), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv2d(in_channels=8, out_channels=4, kernel_size=(3,3), stride=(3,3), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=4, out_channels=8, kernel_size=(3,3), stride=(3,3), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=16, kernel_size=(3,3), stride=(3,3), padding=(0,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=3, kernel_size=(3,3), stride=(1,1), padding=(1,2),dilation=2,output_padding=(0,1)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "#         self.conv1 = nn.ConvTranspose2d(4, 3, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "#         self.conv2 = nn.ConvTranspose2d(3, 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "#         self.conv3 = nn.ConvTranspose2d(2, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(71920, 10)\n",
    "        self.fc2 = nn.Linear(10, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out=nn.Softmax(dim=1)\n",
    "        self.drop = nn.Dropout(p=0.4)\n",
    "        self.bn = nn.BatchNorm1d(10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.conv3(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.relu(self.bn(self.fc1(x)))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cb6438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDAdataset(Dataset):\n",
    "    def __init__(self, root_dir, labels, train = False, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.root_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.listdir(self.root_dir)[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        image = np.asarray(image).astype(np.uint8)\n",
    "        \n",
    "        if self.train:\n",
    "            image = image + np.random.normal(0, 2, image.shape).astype(np.uint8)\n",
    "            \n",
    "        label = np.zeros(3)\n",
    "        label[self.labels[idx]] = 1\n",
    "        if self.transform:\n",
    "            image = self.transform((image/255.).astype(np.float32))\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1b71ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3ed7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(data_dir,\"train\",\"images\")\n",
    "train_labels = np.load(os.path.join(data_dir,\"train\",\"labels\",\"train.npy\"))\n",
    "train_dataset = HDAdataset(train_dir,train_labels,train = True, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "test_dir = os.path.join(data_dir,\"test\",\"images\")\n",
    "test_labels = np.load(os.path.join(data_dir,\"test\",\"labels\",\"test.npy\"))\n",
    "test_dataset = HDAdataset(test_dir,test_labels,train = False,transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "val_dir = os.path.join(data_dir,\"val\",\"images\")\n",
    "val_labels = np.load(os.path.join(data_dir,\"val\",\"labels\",\"val.npy\"))\n",
    "val_dataset = HDAdataset(val_dir,val_labels,train = False,transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2c567e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mYou are working on: \u001b[0mcuda\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "num_epochs = 50\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(BOLD+\"You are working on: \"+END+str(device))\n",
    "\n",
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "classifier = Classifier().to(device)\n",
    "\n",
    "criterion_classifier = nn.CrossEntropyLoss()\n",
    "criterion_ae = nn.MSELoss()\n",
    "optimizer_e = optim.Adam(encoder.parameters(), lr=learning_rate,weight_decay=1e-2)\n",
    "optimizer_d = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "optimizer_c = optim.Adam(classifier.parameters(), lr=learning_rate,weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6460ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(model, (3,1040,1388), batch_size=5, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59edc2ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [1/50]:   8%|█████                                                        | 13/157 [00:18<02:48,  1.17s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ######training#####\n",
    "    losses_ae = []\n",
    "    losses_c = []\n",
    "    count_correct = 0\n",
    "    count_input = 0\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    classifier.train()\n",
    "    for data in tqdm(train_dataloader,desc = \"Train Epoch [%d/%d]\"%(epoch+1,num_epochs)):\n",
    "\n",
    "\n",
    "        \n",
    "        img, labels = data\n",
    "        img = img.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        \n",
    "        optimizer_e.zero_grad()\n",
    "        optimizer_d.zero_grad()\n",
    "        optimizer_c.zero_grad()\n",
    "        \n",
    "        \n",
    "        compressed_img = encoder(img)\n",
    "        rec_img = decoder(compressed_img)\n",
    "        pred_labels = classifier(compressed_img)\n",
    "        \n",
    "        classifier_loss = criterion_classifier(pred_labels, labels)\n",
    "        ae_loss = criterion_ae(img,rec_img)\n",
    "        \n",
    "\n",
    "        classifier_loss.backward(retain_graph=True)\n",
    "        ae_loss.backward(retain_graph=True)\n",
    "\n",
    "        optimizer_e.step()\n",
    "        optimizer_d.step()\n",
    "        optimizer_c.step()\n",
    "\n",
    "        \n",
    "        losses_ae.append(ae_loss.detach().cpu().numpy())\n",
    "        losses_c.append(classifier_loss.detach().cpu().numpy())\n",
    "        \n",
    "        count_input+=pred_labels.detach().cpu().numpy().shape[0]\n",
    "        count_correct+=np.sum((labels.to(\"cpu\").detach().numpy().argmax(axis=1)==pred_labels.to(\"cpu\").detach().numpy().argmax(axis=1)))\n",
    "        del img\n",
    "        del labels\n",
    "        gc.collect();\n",
    "        \n",
    "    ######validation######\n",
    "    losses_ae_val = []\n",
    "    losses_c_val = []\n",
    "    count_correct_val = 0\n",
    "    count_input_val = 0\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    classifier.eval()\n",
    "    for data in tqdm(val_dataloader,desc = \"Validation Epoch [%d/%d]\"%(epoch+1,num_epochs)):\n",
    "\n",
    "        img, labels = data\n",
    "        img = img.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        \n",
    "        compressed_img = encoder(img)\n",
    "        rec_img = decoder(compressed_img)\n",
    "        pred_labels = classifier(compressed_img)\n",
    "        \n",
    "        classifier_loss_val = criterion_classifier(pred_labels, labels)\n",
    "        ae_loss_val= criterion_ae(img,rec_img)\n",
    "        \n",
    "\n",
    "        losses_ae_val.append(ae_loss_val.detach().cpu().numpy())\n",
    "        losses_c_val.append(classifier_loss_val.detach().cpu().numpy())\n",
    "        \n",
    "        count_input_val+=pred_labels.detach().cpu().numpy().shape[0]\n",
    "        count_correct_val+=np.sum((labels.to(\"cpu\").detach().numpy().argmax(axis=1)==pred_labels.to(\"cpu\").detach().numpy().argmax(axis=1)))\n",
    "        del img\n",
    "        del labels\n",
    "        gc.collect();\n",
    "        \n",
    "    # Print loss at end of each epoch\n",
    "    accuracy = (count_correct/count_input)*100.\n",
    "    accuracy_val = (count_correct_val/count_input_val)*100.\n",
    "    print('TRAIN: Epoch [%d/%d], Autoencoder Loss: %.4f, Classifier Loss: %.4f, Accuracy: %.4f in %d input samples \\n'\n",
    "          % (epoch+1, num_epochs, np.mean(losses_ae), np.mean(losses_c),accuracy,count_input))\n",
    "    print('VALIDATION: Epoch [%d/%d], Autoencoder Loss: %.4f, Classifier Loss: %.4f, Accuracy: %.4f in %d input samples\\n'\n",
    "      % (epoch+1, num_epochs, np.mean(losses_ae_val), np.mean(losses_c_val),accuracy_val,count_input_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e1e1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
