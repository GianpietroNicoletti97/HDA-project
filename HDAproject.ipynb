{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37b31e6",
   "metadata": {},
   "source": [
    "## Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ab6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import gc\n",
    "from tqdm import tqdm,trange\n",
    "\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329a92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usefull for better outputs visualization\n",
    "PURPLE = \"\\033[95m\"\n",
    "CYAN = \"\\033[96m\"\n",
    "DARKCYAN = \"\\033[36m\"\n",
    "BLUE = \"\\033[94m\"\n",
    "GREEN = \"\\033[92m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "RED = \"\\033[91m\"\n",
    "BOLD = \"\\033[1m\"\n",
    "UNDERLINE = \"\\033[4m\"\n",
    "END = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7564cc00",
   "metadata": {},
   "source": [
    "### Project settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b42f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_unsplitted = \"dataset/unsplitted\"\n",
    "save_dir_splitted = \"dataset/splitted\"\n",
    "data_dir = \"dataset/splitted\"\n",
    "models_dir =\"models/\" #the resuls will be saved here\n",
    "\n",
    "os.makedirs(models_dir,exist_ok=True)\n",
    "\n",
    "grayscale= True\n",
    "resize_to= (347,260)\n",
    "fft = True\n",
    "\n",
    "learning_rate_e = 0.001\n",
    "learning_rate_d = 0.001\n",
    "#learning_rate_c = 0.00001\n",
    "num_epochs = 50\n",
    "\n",
    "\n",
    "#@DEPRECATED VARIABLE\n",
    "ALREADY_SPLITTED = True #-> we will use only \"LOAD_DATASET\"\n",
    "\n",
    "\n",
    "LOAD_DATASET = True #if true \"ALREADY_SPLITTED\" is ignored\n",
    "\n",
    "#to be implemented:\n",
    "\n",
    "DATASET_ALREADY_DOWNLOADED = True \n",
    "DATASET_ALREADY_UNZIPPED = True\n",
    "\n",
    "LOAD_MODEL = False \n",
    "MODEL_ALREADY_DOWNLOADED = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b6f851",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1385d",
   "metadata": {},
   "source": [
    "### Dataset splitting and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30151b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"CLL\", \"FL\", \"MCL\"]\n",
    "id_to_name = {0:\"CLL\",1:\"FL\",2:\"MCL\"}\n",
    "\n",
    "if(not LOAD_DATASET):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i, cl in enumerate(classes):\n",
    "        path = os.path.join(data_dir_unsplitted, cl)\n",
    "        images = os.listdir(path)\n",
    "        for img in tqdm(images,desc=cl):\n",
    "            img_path = os.path.join(path, img)\n",
    "            img = cv2.imread(img_path).astype(np.uint8)\n",
    "            \n",
    "            data.append(img)\n",
    "            labels.append(i) \n",
    "\n",
    "\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.125, random_state=42)\n",
    "\n",
    "    print(\"Train samples:\", len(train_data))\n",
    "    print(\"Test samples:\", len(test_data))\n",
    "    print(\"Validation samples:\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17f646",
   "metadata": {},
   "source": [
    "### Saving the splitted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd4d2b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING THE DATASET\n",
    "if(not LOAD_DATASET):\n",
    "    #TRAIN\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"train\",\"images\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"train\",\"labels\"),exist_ok=True)\n",
    "\n",
    "     \n",
    "    labels_train_augmented={}\n",
    "    for i, image in enumerate(tqdm(train_data,desc = \"Train\")):                       #images\n",
    "        file_name = str(i)+\"_train\"+\".png\"\n",
    "        \n",
    "        path = os.path.join(save_dir_splitted,\"train\",\"images\",file_name)\n",
    "        labels_train_augmented[file_name] = int(train_labels[i])\n",
    "        cv2.imwrite(path,image.astype(np.float64))\n",
    "        \n",
    "        #flipping\n",
    "        flipped_img = np.fliplr(image)\n",
    "        \n",
    "        file_name = str(i)+\"_flippend_train\"+\".png\"\n",
    "        path = os.path.join(save_dir_splitted,\"train\",\"images\",file_name)\n",
    "        labels_train_augmented[file_name] = int(train_labels[i])\n",
    "        cv2.imwrite(path,flipped_img.astype(np.float64))\n",
    "        \n",
    "        #gaussian noise\n",
    "        noise = np.random.normal(0, 5, img.shape).astype(np.uint8)\n",
    "        noisy_image = image.astype(np.uint8) + noise\n",
    "        \n",
    "        file_name = str(i)+\"_noisy_train\"+\".png\"\n",
    "        path = os.path.join(save_dir_splitted,\"train\",\"images\",file_name)\n",
    "        labels_train_augmented[file_name] = int(train_labels[i])\n",
    "        cv2.imwrite(path,noisy_image.astype(np.float64))\n",
    "    \n",
    "    with open(os.path.join(save_dir_splitted,\"train\",\"labels\",\"train.json\"), 'w') as f:\n",
    "        json.dump(labels_train_augmented, f)#labels\n",
    "\n",
    "    #TEST\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"test\",\"images\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"test\",\"labels\"),exist_ok=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    test_labels_dict = {}\n",
    "    for i, image in enumerate(tqdm(test_data,desc = \"Test\")):                      #images\n",
    "        file_name = str(i)+\"_test\"+\".png\"\n",
    "        path = os.path.join(save_dir_splitted,\"test\",\"images\",file_name)\n",
    "        \n",
    "        cv2.imwrite(path,image.astype(np.float64))\n",
    "        test_labels_dict[file_name] = int(test_labels[i]) \n",
    "\n",
    "    with open(os.path.join(save_dir_splitted,\"test\",\"labels\",\"test.json\"), 'w') as f:\n",
    "        json.dump(test_labels_dict, f)#labels\n",
    "        \n",
    "    #VALIDATION\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"val\",\"images\"),exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir_splitted,\"val\",\"labels\"),exist_ok=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    val_labels_dict = {}\n",
    "    for i, image in enumerate(tqdm(val_data,desc = \"Validation\")):               #images\n",
    "        file_name = str(i)+\"_val\"+\".png\"\n",
    "        path = os.path.join(save_dir_splitted,\"val\",\"images\",file_name)\n",
    "        \n",
    "        cv2.imwrite(path,image.astype(np.float64))\n",
    "        val_labels_dict[file_name] = int(val_labels[i])\n",
    "        \n",
    "    with open(os.path.join(save_dir_splitted,\"val\",\"labels\",\"val.json\"), 'w') as f:\n",
    "        json.dump(val_labels_dict, f)#labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab21346c",
   "metadata": {},
   "source": [
    "### Dataloaders definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a1b860",
   "metadata": {},
   "source": [
    "Notice that the labels are saved into a numpy array, while the images are read directly from the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cb6438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDAdataset(Dataset):\n",
    "    def __init__(self, root_dir, labels, train = False, transform=None, grayscale= False, resize_to= None, fft = False):\n",
    "        self.root_dir = root_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.grayscale=grayscale\n",
    "        self.fft = fft\n",
    "        self.resize_to = resize_to\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.root_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.listdir(self.root_dir)[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        \n",
    "        if self.grayscale or self.fft:\n",
    "            image = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if self.train:\n",
    "            image = image.astype(np.uint8) + np.random.normal(0, 2, image.shape).astype(np.uint8)\n",
    "        \n",
    "        if self.resize_to is not None:\n",
    "            image = cv2.resize(image, resize_to, interpolation= cv2.INTER_LINEAR)\n",
    "            \n",
    "        if self.fft:\n",
    "            image = np.fft.fft2(image)\n",
    "            fshift = np.fft.fftshift(image)\n",
    "            magnitude_spectrum = 20*np.log(np.abs(fshift))\n",
    "            image = magnitude_spectrum\n",
    "        \n",
    "\n",
    "        \n",
    "        #labels smoothing\n",
    "        label = 0.1*np.ones(3)\n",
    "        label[self.labels[img_name]] = 0.8\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform((image/255.).astype(np.float32))\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26887536",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(LOAD_DATASET and not DATASET_ALREADY_DOWNLOADED):\n",
    "    print(\"download the dataset\")\n",
    "    urllib.request.urlretrieve(\"\", \"dataset.zip\")\n",
    "    \n",
    "if(LOAD_DATASET and not DATASET_ALREADY_UNZIPPED):\n",
    "    with zipfile.ZipFile(\"dataset.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./dataset/\")\n",
    "\n",
    "\n",
    "#convert to tensor each image\n",
    "transform = transforms.Compose([transforms.ToTensor()])    \n",
    "\n",
    "#TRAIN DATASET AND DATALOADER\n",
    "train_dir = os.path.join(data_dir,\"train\",\"images\")#images\n",
    "train_labels_dir = os.path.join(data_dir,\"train\",\"labels\",\"train.json\")\n",
    "with open(train_labels_dir) as f:\n",
    "    train_labels = json.load(f)#labels\n",
    "\n",
    "train_dataset = HDAdataset(train_dir,train_labels,train = False,grayscale = grayscale, resize_to= resize_to,\n",
    "                           transform=transform,fft = fft)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True,drop_last=True)\n",
    "\n",
    "#TEST DATASET AND DATALOADER\n",
    "test_dir = os.path.join(data_dir,\"test\",\"images\")#images\n",
    "test_labels_dir = os.path.join(data_dir,\"test\",\"labels\",\"test.json\")\n",
    "with open(test_labels_dir) as f:\n",
    "    test_labels = json.load(f)#labels\n",
    "\n",
    "test_dataset = HDAdataset(test_dir,test_labels,train = False,grayscale = grayscale, resize_to= resize_to,\n",
    "                           transform=transform,fft = fft)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "#VALIDATION DATASET AND DATALOADER\n",
    "val_dir = os.path.join(data_dir,\"val\",\"images\")#images\n",
    "\n",
    "val_labels_dir = os.path.join(data_dir,\"val\",\"labels\",\"val.json\")\n",
    "with open(val_labels_dir) as f:\n",
    "    val_labels = json.load(f)#labels\n",
    "\n",
    "val_dataset = HDAdataset(val_dir,val_labels,train = False,grayscale = grayscale, resize_to= resize_to,\n",
    "                           transform=transform,fft = fft)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5599d",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "915f464f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684cf15",
   "metadata": {},
   "source": [
    "### Models definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f54470d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3,3), stride=(3,3), padding=(1,1)),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), stride=(3,3), padding=(1,1)),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(256,20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(20,10),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.classification = nn.Sequential(\n",
    "            nn.Linear(10,5),\n",
    "            nn.BatchNorm1d(5),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(5,3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x, self.classification(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10,20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.LeakyReLU(),\n",
    "            #nn.Dropout(p=0.1),\n",
    "            nn.Linear(20,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            #nn.Dropout(p=0.1),\n",
    "            nn.Unflatten(-1, (64,2,2)),\n",
    "            nn.Upsample(size=(4, 5), mode='bilinear'),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=(3,3), stride=(3,3), padding=(1,1)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            #nn.Dropout(p=0.1),\n",
    "            nn.Upsample(size=(22, 29), mode='bilinear'),\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=(3,3), stride=(3,3), padding=(0,0)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            #nn.Dropout(p=0.1),\n",
    "            nn.Upsample(size=(65, 87), mode='bilinear'),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=(3,3), stride=(2,2), padding=(1,1),dilation=1,output_padding=(0,0)),\n",
    "            nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=(3,3), stride=(2,2), padding=(0,0),dilation=1,output_padding=(1,0)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2c567e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mYou are working on: \u001b[0mcuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(BOLD+\"You are working on: \"+END+str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f5a37",
   "metadata": {},
   "source": [
    "### Loading of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9139a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(LOAD_MODEL and not MODEL_ALREADY_DOWNLOADED):\n",
    "    print(\"download the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6460ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not LOAD_MODEL):\n",
    "    encoder = Encoder().to(device)\n",
    "    decoder = Decoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08164ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = np.random.uniform(0,2,(3,20)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34754da9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x20 and 10x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15228\\651675284.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15228\\793413180.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     72\u001b[0m         )\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x20 and 10x20)"
     ]
    }
   ],
   "source": [
    "out = decoder(torch.tensor(inp).to(device))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1214cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(encoder, (1, 260, 347),batch_size=3)\n",
    "# summary(decoder, (1,60),batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931580de",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b91a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader,name):\n",
    "    \n",
    "    losses_ae = []\n",
    "    losses_c = []\n",
    "    count_correct = 0\n",
    "    count_input = 0\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    #classifier.eval()\n",
    "    for data in tqdm(dataloader,desc = name +\" set\"):\n",
    "\n",
    "        img, labels = data\n",
    "        img = img.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "\n",
    "        compressed_img,pred_labels = encoder(img)\n",
    "        rec_img = decoder(compressed_img)\n",
    "        #pred_labels = classifier(compressed_img)\n",
    "\n",
    "        classifier_loss = criterion_classifier(pred_labels, labels)\n",
    "        ae_loss= criterion_ae(img,rec_img)\n",
    "\n",
    "\n",
    "        losses_ae.append(ae_loss.detach().cpu().numpy())\n",
    "        losses_c.append(classifier_loss.detach().cpu().numpy())\n",
    "\n",
    "        count_input+=pred_labels.detach().cpu().numpy().shape[0]\n",
    "        count_correct+=np.sum((labels.to(\"cpu\").detach().numpy().argmax(axis=1)==pred_labels.to(\"cpu\").detach().numpy().argmax(axis=1)))\n",
    "        del img\n",
    "        del labels\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect();\n",
    "\n",
    "    # Print loss at end of each epoch\n",
    "    accuracy= (count_correct/count_input)*100.\n",
    "    print(name,' Autoencoder Loss: %.4f, Classifier Loss: %.4f, Accuracy: %.4f in %d input samples\\n'\n",
    "      % (np.mean(losses_ae), np.mean(losses_c),accuracy,count_input))\n",
    "    \n",
    "    return np.mean(losses_ae),np.mean(losses_c),accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17514270",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_classifier = nn.CrossEntropyLoss()\n",
    "criterion_ae = nn.MSELoss()\n",
    "\n",
    "def train(train_dataloader,val_dataloader):\n",
    "    optimizer_e = optim.Adam(encoder.parameters(), lr=learning_rate_e,weight_decay=1e-2)\n",
    "    optimizer_d = optim.Adam(decoder.parameters(), lr=learning_rate_d,weight_decay=1e-2)\n",
    "\n",
    "    \n",
    "    \n",
    "    best_epoch = 0 #(based on the validation accuracy)\n",
    "    best_accuracy = -np.inf\n",
    "    \n",
    "    #for plotting\n",
    "    train_loss_ae_trend = []\n",
    "    train_loss_c_trend = []\n",
    "    train_accuracy_trend =[]\n",
    "    val_loss_ae_trend = []\n",
    "    val_loss_c_trend = []\n",
    "    val_accuracy_trend = []\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        ######training#####\n",
    "        losses_ae = []\n",
    "        losses_c = []\n",
    "        count_correct = 0\n",
    "        count_input = 0\n",
    "\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        \n",
    "        #train autoencoder for one epoch\n",
    "        for data in tqdm(train_dataloader,desc = \"Train autoencoder Epoch [%d/%d]\"%(epoch+1,num_epochs)):\n",
    "\n",
    "            img, labels = data\n",
    "            img = img.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer_e.zero_grad()\n",
    "            optimizer_d.zero_grad()\n",
    "\n",
    "            compressed_img,pred_labels= encoder(img)\n",
    "            rec_img = decoder(compressed_img)\n",
    "\n",
    "    \n",
    "            ae_loss = criterion_ae(img,rec_img)\n",
    "            ae_loss2 = 0.1*ae_loss\n",
    "            \n",
    "            ae_loss2.backward()\n",
    "            optimizer_e.step()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            \n",
    "\n",
    "        #train classifier for one epoch    \n",
    "        for data in tqdm(train_dataloader,desc = \"Train classifier Epoch [%d/%d]\"%(epoch+1,num_epochs)):\n",
    "\n",
    "            img, labels = data\n",
    "            img = img.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer_e.zero_grad()\n",
    "            \n",
    "\n",
    "            compressed_img,pred_labels= encoder(img)\n",
    "            \n",
    "            \n",
    "\n",
    "            classifier_loss = criterion_classifier(pred_labels, labels)\n",
    "\n",
    "            classifier_loss.backward()\n",
    "            optimizer_e.step()\n",
    "            \n",
    "            losses_ae.append(ae_loss.detach().cpu().numpy())\n",
    "            losses_c.append(classifier_loss.detach().cpu().numpy())\n",
    "\n",
    "            count_input+=pred_labels.detach().cpu().numpy().shape[0]\n",
    "            count_correct+=np.sum((labels.to(\"cpu\").detach().numpy().argmax(axis=1)==pred_labels.to(\"cpu\").detach().numpy().argmax(axis=1)))\n",
    "            del img\n",
    "            del labels\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect();\n",
    "            \n",
    "        accuracy = (count_correct/count_input)*100.\n",
    "        \n",
    "        print('TRAIN: Epoch [%d/%d], Autoencoder Loss: %.4f, Classifier Loss: %.4f, Accuracy: %.4f in %d input samples \\n'\n",
    "              % (epoch+1, num_epochs, np.mean(losses_ae), np.mean(losses_c),accuracy,count_input))\n",
    "        \n",
    "        train_loss_ae_trend.append(np.mean(losses_ae))\n",
    "        train_loss_c_trend.append(np.mean(losses_c))\n",
    "        train_accuracy_trend.append(accuracy)\n",
    "        \n",
    "        ######validation######\n",
    "        ae,c,acc = test(val_dataloader,\"VALIDATION\")\n",
    "        \n",
    "        val_loss_ae_trend.append(ae)\n",
    "        val_loss_c_trend.append(c)\n",
    "        val_accuracy_trend.append(acc)\n",
    "        \n",
    "        if acc > best_accuracy:\n",
    "            print(GREEN+\"Saved Model: \"+END+UNDERLINE+\"best validation accuracy reached\\n\\n\"+END)\n",
    "            best_epoch = epoch+1\n",
    "            torch.save(encoder.state_dict(), os.path.join(models_dir,\"best_e.pt\"))\n",
    "            torch.save(decoder.state_dict(), os.path.join(models_dir,\"best_d.pt\"))\n",
    "            \n",
    "            best_accuracy = acc\n",
    "\n",
    "    return np.array([[train_loss_ae_trend,train_loss_c_trend,train_accuracy_trend],\n",
    "            [val_loss_ae_trend,val_loss_c_trend,val_accuracy_trend],best_epoch],dtype=np.object_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59edc2ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train autoencoder Epoch [1/50]: 100%|██████████████████████████████████████████████████| 78/78 [00:33<00:00,  2.33it/s]\n",
      "Train classifier Epoch [1/50]: 100%|███████████████████████████████████████████████████| 78/78 [00:37<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Epoch [1/50], Autoencoder Loss: 0.0569, Classifier Loss: 1.0773, Accuracy: 47.4359 in 780 input samples \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALIDATION set: 100%|████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION  Autoencoder Loss: 0.0564, Classifier Loss: 1.0806, Accuracy: 44.7368 in 38 input samples\n",
      "\n",
      "\u001b[92mSaved Model: \u001b[0m\u001b[4mbest validation accuracy reached\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train autoencoder Epoch [2/50]: 100%|██████████████████████████████████████████████████| 78/78 [00:29<00:00,  2.65it/s]\n",
      "Train classifier Epoch [2/50]: 100%|███████████████████████████████████████████████████| 78/78 [00:37<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Epoch [2/50], Autoencoder Loss: 0.0486, Classifier Loss: 1.0505, Accuracy: 55.8974 in 780 input samples \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALIDATION set: 100%|████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION  Autoencoder Loss: 0.0465, Classifier Loss: 1.0745, Accuracy: 39.4737 in 38 input samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train autoencoder Epoch [3/50]: 100%|██████████████████████████████████████████████████| 78/78 [00:29<00:00,  2.63it/s]\n",
      "Train classifier Epoch [3/50]: 100%|███████████████████████████████████████████████████| 78/78 [00:36<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Epoch [3/50], Autoencoder Loss: 0.0403, Classifier Loss: 1.0354, Accuracy: 60.2564 in 780 input samples \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALIDATION set: 100%|████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION  Autoencoder Loss: 0.0399, Classifier Loss: 1.0884, Accuracy: 36.8421 in 38 input samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train autoencoder Epoch [4/50]: 100%|██████████████████████████████████████████████████| 78/78 [00:30<00:00,  2.59it/s]\n",
      "Train classifier Epoch [4/50]: 100%|███████████████████████████████████████████████████| 78/78 [00:37<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Epoch [4/50], Autoencoder Loss: 0.0351, Classifier Loss: 1.0234, Accuracy: 62.8205 in 780 input samples \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALIDATION set: 100%|████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION  Autoencoder Loss: 0.0345, Classifier Loss: 1.0977, Accuracy: 36.8421 in 38 input samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train autoencoder Epoch [5/50]: 100%|██████████████████████████████████████████████████| 78/78 [00:30<00:00,  2.52it/s]\n",
      "Train classifier Epoch [5/50]:  42%|█████████████████████▌                             | 33/78 [00:16<00:22,  2.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15228\\3484711082.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mLOAD_MODEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"results.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15228\\2408060676.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_dataloader, val_dataloader)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mclassifier_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0moptimizer_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mlosses_ae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mae_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'differentiable'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             adam(params_with_grad,\n\u001b[0m\u001b[0;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m     func(params,\n\u001b[0m\u001b[0;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcapturable\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if(not LOAD_MODEL):\n",
    "    results = train(train_dataloader,val_dataloader)\n",
    "    np.save(os.path.join(models_dir,\"results.npy\"),results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c13757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#always load the best model after the training\n",
    "encoder =  Encoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "encoder.load_state_dict(torch.load(os.path.join(models_dir,\"best_e.pt\")))\n",
    "encoder.eval();\n",
    "decoder.load_state_dict(torch.load(os.path.join(models_dir,\"best_d.pt\")))\n",
    "decoder.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dcbf59",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17494b39",
   "metadata": {},
   "source": [
    "Notice that the value of the losses and the accuraciesfor the train set are computed during the training, so they are not very accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4bfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_no_noise = HDAdataset(train_dir,train_labels,train = False, transform=transform)\n",
    "# train_dataloader_no_noise = DataLoader(train_dataset_no_noise, batch_size=64, shuffle=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641cfb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(train_dataloader,\"TRAIN\")\n",
    "test(val_dataloader,\"VALIDATION\")\n",
    "test(test_dataloader,\"TEST\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eed0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
